# -*- coding: utf-8 -*-
"""language detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBVWXpcCFHDboY-CdUsFwEpW1CZd5U-C
"""

from google.colab import drive

drive.mount('/content/gdrive')

import os
os.chdir('/content/gdrive/MyDrive/language detection')
os.getcwd()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('Language Detection.csv')

df.head()

df.tail()

df.shape

df['Language'].value_counts()

df['Language'].value_counts().plot(kind='bar')

plt.figure(figsize=(20,20))
sns.countplot(df['Language'])

import nltk #natural language toolkit
nltk.download('stopwords')
from nltk.corpus import stopwords #provide functions that can be used to read corpus files in a variety of formats.
from nltk.stem import PorterStemmer #a process for removing the commoner morphological and inflexional endings from words in English
import re #RegEx: Regular Expression, a sequence of characters that forms a search pattern. used to check if a string contains the specified search pattern.

"""####testing"""

set(stopwords.words('english'))

ps = PorterStemmer()
test = [ 'This is the first document.','This is the second second document.', 'And the third one.','Is this the first document?', 'i have SEEN a lot of different people 19 working harder than ever ' ]
corpus1=[]

for i in range(len(test)):
  test1 = re.sub("[^a-zA-Z]", ' ',test[i])
  test1=test1.lower()
  test1=test1.split()
  test1=[ps.stem(word) for word in test1 if set(stopwords.words('english'))]
  print(test1)
  test1 = ' '.join(test1)
  print(test1)
  corpus1.append(test1)

print(corpus1)

from sklearn.feature_extraction.text import CountVectorizer #Convert a collection of text documents to a matrix of token counts.
cv = CountVectorizer(max_features=10000)
vectorizer = CountVectorizer()
Y = vectorizer.fit_transform(corpus1).toarray()

Y

"""from nltk.corpus.reader import wordlist

remove any character that IS NOT a-z OR A-Z

lower case

split words

remove stop words and endings

rejoin word

append to the list

### next
"""

ps = PorterStemmer()
corpus=[] #Define an empty corpus list, that can be used to store all the text after cleaning.

for i in range(len(df['Text'])):
    
    rev = re.sub("^[a-zA-Z]",' ', df['Text'][i]) 
    rev = rev.lower()
    rev = rev.split()
    rev = [ps.stem(word) for word in rev if set(stopwords.words())]
    rev = ' '.join(rev)
    corpus.append(rev)

"""On parle de corpus pour désigner l'aspect normatif de la langue : sa structure et son code en particulier.

Language classifications rely upon using a primer of specialized text called a 'corpus.' There is one corpus for each language the algorithm can identify. In summary, the input text is compared to each corpus, and pattern matching is used to identify the strongest correlation to a corpus.

In Cleaning Process the next step is to convert the list of the sentence (corpus) into vectors so that we can feed this data into our machine learning model. for converting the text into vectors we are going to use a bag of words which is going to convert the text into binary form.’
"""

from sklearn.feature_extraction.text import CountVectorizer #Convert a collection of text documents to a matrix of token counts.
cv = CountVectorizer(max_features=10000) #Creating an object for the count vectorizer with max features as 10000, means we are only fetching the top 10000 columns.
X = cv.fit_transform(corpus).toarray() # Using CV we are fitting are corpus and also transforming it into vectors.

cv

X

X.shape

from sklearn.preprocessing import LabelEncoder  #Encode target labels with value between 0 and n_classes-1.
label = LabelEncoder()
y = label.fit_transform(df['Language'])

y

label.classes_

data1 = pd.DataFrame(np.c_[corpus,y],columns=['Sentence','Language'])

data1

from sklearn.model_selection import train_test_split #Split arrays or matrices into random train and test subsets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.naive_bayes import MultinomialNB

#fit() : used for generating learning model parameters from training data

#model creation with Fitting the model to the training sets
classifier = MultinomialNB().fit(X_train,y_train)

#prediction
pred = classifier.predict(X_test)

pred

y_test

"""confusion_matrix :
Compute confusion matrix to evaluate the accuracy of a classification.
By definition a confusion matrix C is such that C(i,j) is equal to the number of observations known to be in group i and predicted to be in group j.
"""

from sklearn.metrics import accuracy_score,confusion_matrix
print(accuracy_score(y_test,pred))
print(confusion_matrix(y_test,pred))

plt.figure(figsize=(20,20))
sns.heatmap(confusion_matrix(y_test,pred),annot=True,cmap=plt.cm.Accent)

fnl = pd.DataFrame(np.c_[y_test,pred],columns=['Actual','Predicted'])
fnl

"""After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain:
joblib allows to save data structures in a file to be able to load them afterwords (for exp a ML modele). more efficient for numpy structures than pickle.
dump to save data structures.
"""

import joblib

joblib.dump(classifier , 'language_identification.sav')
#Persist an arbitrary Python object into one file.

model = joblib.load('language_identification.sav')

def test_model(test_sentence):
    languages = {
    'Arabic' : 0,
    'Danish' : 1,
    'Dutch' : 2,
    'English' : 3,
    'French' : 4,
    'German' : 5,
    'Greek' : 6,
    'Hindi' : 7, 
    'Italian' : 8, 
    'Kannada' : 9, 
    'Malayalam' : 10,
    'Portugese' : 11,
    'Russian' : 12,
    'Spanish' : 13,
    'Swedish' : 14,
    'Tamil' : 15,
    'Turkish' : 16
    }
    
    
    
    
    rev = re.sub('^[a-zA-Z]',' ',test_sentence)
    rev = rev.lower()
    rev = rev.split()
    rev = [ps.stem(word) for word in rev if word not in set(stopwords.words())]
    rev = ' '.join(rev)
    
    rev = cv.transform([rev]).toarray()
    
    output = model.predict(rev)[0]
    
    keys = list(languages)
    values = list(languages.values())
    position = values.index(output)
    
    output = keys[position]
    
    print(output)

test_model('Ich traf Annalisa in Colosseo und sie sagte mir, sie hätte gerne Spaghetti und ein Stück Pizza')

pd.DataFrame(np.c_[df['Text'],df['Language'],y],columns=['Sentence','Language','Encoded'])

